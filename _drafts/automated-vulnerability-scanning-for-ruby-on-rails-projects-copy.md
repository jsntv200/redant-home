---
layout: post
title: Securing your SaaS application data
permalink: "/automated-testing/ruby-on-rails-devops/securing-your-saas-application-data/"
type: ideas
categories:
- automated-testing
- ruby-on-rails-devops
- tool-reviews
author: Ben Still
description: We've been using Brakeman for vulnerability scanning for our Ruby on
  Rails projects. Brakeman analyses code to detect possible security issues with an
  application. Basically, there is this big list of possible ways an app can be compromised,
  and vulnerability checkers run through these
keywords: vulnerability scanning
image_small: "/assets/uploads/2012/m-lo-small.jpg"
image: "/assets/uploads/2012/m-lo.jpg"
excerpt_short: Vulnerability scanning software Ruby on Rails analysis
excerpt_long: We've been using Brakeman for vulnerability scanning for our Ruby on
  Rails projects. Brakeman analyses code to detect possible security issues with an
  application. Basically, there is this big list of possible ways an app can be compromised,
  and vulnerability checkers run through these
tags: []
time: ''
redirect_from:
- "/automated-testing/ruby-on-rails-devops/"
- "/blog/vulnerability-scanning-ruby-on-rails/"
date_published: ! ' 2012-09-25'
publisher: Red Ant

---
Imagine for a moment you’re a bigshot at an Australian Telco. And you’ve just found out that somehow, a gazillion customer records are now for sale on the interweb. That gut wrenching sensation when it becomes apparent that no one knows *precisely* what’s been compromised. And instead of just a fine and some bad press, you now seem to be on the hook to fund new drivers licences and passports for a significant number of people. Some of who aren’t even customers.

This can happen at all different stages - you might be a tiny Fintech or that Aussie Telco. So before it happens, what are some pragmatic steps you can take?

### **Don’t keep the data for longer than you need**

The most obvious thing is to not hang on to that honeypot data. Really obvious in hindsight, of course. Data has value, sure. And when someone from your marketing team comes back from a conference spouting about Big Data and the need for a “Data Lake”, it might seem like discarding data means destroying value. Because someone in marketing is learning R and is really keen to have a poke around.

But that data is also a liability. As it gets older, the chance that it can be used for remarketing reduces. So if you really can’t destroy it, depersonalise as much as you can. Make sure it isn’t hanging around on caches, backups or in ancillary audit / analytics tools like GA.

Now brace for all the people that then “need” that data. Spoiler alert: they probably don’t, but it would be interesting. Make sure you’ve already got stakeholder buy in around the liability aspect.

### **Don’t even keep it in the first place**

Even better is to not store the data in the first place. This has become fairly common for Card data, to try to avoid getting in scope for PCI (another spoiler alert: you might still be). Instead of storing that data in your system, it lives somewhere else. You store a token in your system, which then allows you to reference that raw data which is now someone else’s problem.

But if your app does need to grab this data quickly, or query it regularly, this approach has some pretty severe limitations.

### **Automate the things**

The next step is around automation. I cannot stress this one enough. Don’t rely on some developer (or maybe you) to remember to do something. Or a complex chain of things. And it really doesn’t matter how awesome your task list app is or how much coffee you drink. Once something is automated, we can test that it ran, record this in an audit trail, and if it breaks or fails, we can generate an alert.

Automation is the codification of a process, so by automating we’re forcing the organisation to create and formalise various processes. One of those might be to delete any customer data greater than X days old. When a task comes up that involves your honeypot of data, instead of just doing it, a sensible approach is to create an automation and schedule that.

An automation might run as part of your deploy process, so each time new code is pushed up some routines happen. They might run as part of the development cycle, so at the start of each sprint or release, something happens. Others might be on time triggers at the end of day, month or quarter.

### **Obfuscate**

We also need to be pragmatic and acknowledge that some people will need that sensitive data. Say a new feature has been developed, if you don’t have data how would anyone meaningfully test it? Seed data can work sometimes, but that tends to be what a developer expects the data to be, rather than what real customers actually do.

If you take the “stick head deep in sand” approach to this, people will probably work out a way to circumvent and walk around your pile of sand. So that’s not ideal. A better approach might be to obfuscate PII data as part of an automation. Maybe each night, the UAT server gets updated with a fresh set of obfuscated data, which has no PII, everyone lives on random streets and all the passwords are set to 1234. A developer can also check out a recent but smaller subset.

### **Audit access to the honeypot**

Some people will need to get access to that honeypot data as part of their job. One approach here is to change the steps involved. So rather than the search “show me our top 100 customers” coming back with a big search results page of everything, it just shows customer IDs and firstname. Clicking on each shows details. That achieves two things: it first avoids an unintentional exposure of a wide search, and it also allows an audit of who saw what data, or apply finer control via permissions at this point. It also helps move thinking around which parts of the data set are sensitive, and who should see what.

### So in summary:

* Data can be a liability. Set up processes to get rid of it as soon as possible
* Try not to collect it in the first place
* Automate everything. Then automate some more.